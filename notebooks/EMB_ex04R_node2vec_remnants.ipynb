{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Created: 2022-09-26\n",
    "- Last modified: 2022-09-28\n",
    "- Author: Daniel Kaiser\n",
    "- Project Identifiers: EMB_ex04R\n",
    "- Last read and understood by: ???\n",
    "- Changelog:\n",
    "    - (2022-09-26) Initialized notebook\n",
    "    - (2022-09-26) Added processing on split edgelist duplexes\n",
    "    - (2022-09-28) Removed split duplex processed, added raw multiplex processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Experiment ID and project**</u>\n",
    "\n",
    "EMB_ex04R\n",
    "\n",
    "<u>**Date designed and version**</u>\n",
    "\n",
    "- (2022-09-26) v1.0\n",
    "- (2022-09-28) v2.0\n",
    "\n",
    "\n",
    "<u>**Date conducted**</u>\n",
    "\n",
    "- (2022-09-28) v2.0\n",
    "\n",
    "<u>**Researcher(s)**</u>\n",
    "\n",
    "Daniel Kaiser\n",
    "\n",
    "<u>**Description**</u>\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "This experiment is designed to explore the accuracy of multiplex reconstruction utilizing Euclidean distance as a proxy for the likelihood of edge-layer placement. Embedding the \"remnant\" of each layer of a XOR-aggregated duplex, an edge to be classified from the aggregate is predicted to originate from the layer whose remnant embedding yields a smaller distance of the edge endpoints.\n",
    "\n",
    "**Hypothesis (if applicable)**\n",
    "\n",
    "The expected outcome of this experiment is an AUROC curve of classification that is an increasing function of PFI, decreasing function of degree homogeneity and edge correlation. Note this matches the high-level qualitative results of the naive Bayes approach used in earlier work. (Project identifier \"MEC\")\n",
    "\n",
    "<u>**Data & Code**</u>\n",
    "\n",
    "**Prerequisite data (if applicable)**\n",
    "\n",
    "This experiment measures AUROC on a small collection of well-researched dyadic multiplexes. These are the datasets:\n",
    "\n",
    "- arXiv collaborations (N nodes, 13 layers)\n",
    "- _C. Elegans_ connectome (279 nodes, 3 layers)\n",
    "- _Drosophila_ genetic interactions (N nodes, 7 layers)\n",
    "- London transportation (N nodes, 3 layers)\n",
    "\n",
    "Additional paramaters include:\n",
    "\n",
    "- largest_component; flag if remnants should be restricted to largest component (forced with Mercator, optional here to maintain consistency)\n",
    "- pfi; relative size of multiplex topology observed _a priori_\n",
    "- metric; probability model selected (see Procedure#5)\n",
    "\n",
    "\n",
    "**Resultant data**\n",
    "\n",
    "The resultant data is in the form of a dataframe with columns:\n",
    "- system -- name of multiplex\n",
    "- left -- layer id of induced duplex layer one\n",
    "- right -- layer id of induced duplex layer two\n",
    "- largest_component -- boolean flag indicating restriction to largest component\n",
    "- pfi -- relative size of _a priori_ subtensor\n",
    "- metric -- string indicator of likelihood model used\n",
    "- Accuracy  -- accuracy of classifications\n",
    "- AUROC -- area under ROC curve of prediction scores\n",
    "- dimensions -- embedding dimension\n",
    "- walk_length  -- length of samples walks\n",
    "- num_walks -- number of walks per node\n",
    "- workers -- cores (threads?) per random walk process\n",
    "- window -- ??\n",
    "- min_count -- ??\n",
    "- batch_words -- ??\n",
    "\n",
    "**Code**\n",
    "\n",
    "This experiment relies on the custom modules `synthetic.py` & `utils.py` from the `src/` directory, or functions in this document. All code from `src/` is original or only slightly modified MEC code from @Filippo Radicchi.\n",
    "\n",
    "@Elior Cohen and node2vec's [GitHub repo](https://github.com/eliorc/node2vec) inspired the function `embed_system`.\n",
    "\n",
    "Some package version updates in Gensim forced some adjustments - thanks to @555wen on [this StackExchange post](https://stackoverflow.com/questions/67413006/typeerror-init-got-an-unexpected-keyword-argument-size) from @yinky for pointing out a parameter name change. In the site-packages for node2vec, `node2vec.py:175` was altered from `if 'size' not in skip_gram_params:` to `if 'vector_size' not in skip_gram_params:`and `node2vec.py:176` was altered from `skip_gram_params['size'] = self.dimensions` to `skip_gram_params['vector_size'] = self.dimensions`.\n",
    "\n",
    "<u>**Procedure**</u>\n",
    "\n",
    "The following procedure was conducted to run this experiment and is sufficient to replicate the workflow along with code described in the \"Code\" section above.\n",
    "1. Load duplex, $\\mathcal{M} = (V, (\\alpha, \\beta))$\n",
    "2. Aggregate duplex into a monoplex, $G$, and observe some partial information for each layer $\\mathcal{O}_{\\alpha}, \\mathcal{O}_{\\beta}$\n",
    "3. Create remnants by $\\mathcal{R}_{\\alpha} = G \\setminus \\mathcal{O}_{\\beta}, \\mathcal{R}_{\\beta} = G \\setminus \\mathcal{O}_{\\alpha}$\n",
    "4. Embed $\\mathcal{R}_{\\alpha}, \\mathcal{R}_{\\beta}$ separately using node2vec. Call the resultant embedded points $\\mathcal{D}_{\\alpha}, \\mathcal{D}_{\\beta}$.\n",
    "5. For $(i, j) \\in G \\setminus (\\mathcal{O}_{\\alpha} \\cup \\mathcal{O}_{\\beta})$, classify $(i,j)$ as having originated from layer $\\alpha$ ($\\beta$) with (one minus) the following probability:\n",
    "    $$\n",
    "        P((i,j) \\in \\alpha) = \\frac{\n",
    "            \\frac{1}{d_{\\alpha}^{\\varepsilon}(i,j)}\n",
    "        }{\n",
    "            \\frac{1}{d_{\\alpha}^{\\varepsilon}(i,j)} + \\frac{1}{d_{\\beta}^{\\varepsilon}(i,j)}\n",
    "        }\n",
    "    $$\n",
    "    or\n",
    "    $$\n",
    "        P((i,j) \\in \\alpha) = \\frac{\n",
    "            e^{-d_{\\alpha}^{\\varepsilon}(i,j)}\n",
    "        }{\n",
    "            e^{-d_{\\alpha}^{\\varepsilon}(i,j)} + e^{-d_{\\beta}^{\\varepsilon}(i,j)}\n",
    "        }\n",
    "    $$\n",
    "\n",
    "6. Calling the true originating layer of $(i, j) \\in G \\setminus (\\mathcal{O}_{\\alpha} \\cup \\mathcal{O}_{\\beta})$ as $l_{ij}$, measure $\\| l - \\hat{l} \\|$\n",
    "    - NOTE: Since this is a binary classification problem, we actually elect to use the AUROC of the classification of $(i,j) \\mapsto \\hat{l}_{ij}$ with ground truth $l_{ij}$. The vector norm is simply a more general form of measuring \"difference\" in our classifications and the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical foundations and symbol/prior result guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will eventually ill in with background from project \"MEC\" but this seemed like a waste of time at the current moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library ---\n",
    "import sys  # System pathing\n",
    "import os\n",
    "import yaml  # Configuration files\n",
    "from datetime import datetime  # Timestamping data\n",
    "from copy import deepcopy  # Parameter grid processing\n",
    "from itertools import product\n",
    "\n",
    "# --- Scientific ---\n",
    "import numpy as np  # General computational tools\n",
    "from sklearn import metrics  # Measuring classifier performance\n",
    "\n",
    "# --- Network science ---\n",
    "import networkx as nx  # General network tools\n",
    "from node2vec import Node2Vec as N2V  # Embedding tools\n",
    "\n",
    "# --- Project source code ---\n",
    "sys.path.append(\"../src/\")\n",
    "from synthetic import *  # Custom synthetic benchmarks\n",
    "from utils import *\n",
    "\n",
    "# --- Data handling and visualization ---\n",
    "import pandas as pd  # Dataframe tools\n",
    "from tabulate import tabulate  # Pretty printing for dataframes\n",
    "\n",
    "import seaborn as sns  # Easier plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# --- Miscellaneous ---\n",
    "from tqdm.auto import tqdm  # Progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Other aliases ---\n",
    "today = datetime.today\n",
    "accuracy = metrics.accuracy_score\n",
    "auroc = metrics.roc_auc_score\n",
    "\n",
    "# --- Visualization ---\n",
    "rc_dict = {\n",
    "  \"savefig.dpi\": 900,  # Saved figure dots-per-inch. 600 is \"HD\"\n",
    "  \"savefig.facecolor\": \"white\",  # This, combined with transparent setting, keeps saved figs from looking like trash on dark backgrounds \n",
    "  \"savefig.transparent\": False,\n",
    "  \"figure.figsize\": (14, 10),  # Default (width, height) of figure\n",
    "}\n",
    "plt.rcParams.update(rc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Versioning -\n",
    "project_id = \"EMB_ex04R\"\n",
    "current_version = \"v2.0\"\n",
    "researcher = \"DK\"\n",
    "date = today().strftime(\"%Y%m%d\")\n",
    "\n",
    "df_template = f\"dataframe_{project_id}{current_version}_{researcher}_{date}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_system(params):\n",
    "    # Process parameters\n",
    "    pfi = params[\"pfi\"]\n",
    "    system = params[\"system\"]\n",
    "    left = params[\"left\"]\n",
    "    right = params[\"right\"]\n",
    "\n",
    "    # Form \"raw\" duplex\n",
    "    multiplex = read_file(f\"../data/input/raw/multiplex_network-{system}.edgelist\")\n",
    "\n",
    "    # Split into layers\n",
    "    G, H = duplex_network(multiplex, left, right)\n",
    "\n",
    "    # Observe partial information\n",
    "    R_G, R_H, testset = partial_information(G, H, pfi)\n",
    "\n",
    "    # Restrict to largest connected component (if specified)\n",
    "    if params[\"largest_component\"]:\n",
    "        R_G_ = nx.Graph()\n",
    "        R_H_ = nx.Graph()\n",
    "        R_G_.add_nodes_from(R_G.nodes())\n",
    "        R_H_.add_nodes_from(R_H.nodes())\n",
    "\n",
    "        maxcc_R_G = max(nx.connected_components(R_G), key=len)\n",
    "        maxcc_R_H = max(nx.connected_components(R_H), key=len)\n",
    "\n",
    "        edges_R_G_ = set(R_G.subgraph(maxcc_R_G).edges())\n",
    "        edges_R_H_ = set(R_H.subgraph(maxcc_R_H).edges())\n",
    "        R_G_.add_edges_from(edges_R_G_)\n",
    "        R_H_.add_edges_from(edges_R_H_)\n",
    "\n",
    "        testset = {\n",
    "            edge: gt_\n",
    "            for edge, gt_ in testset.items()\n",
    "            if edge in edges_R_H_ | edges_R_H_\n",
    "        }\n",
    "\n",
    "    return G, H, R_G, R_H, testset\n",
    "\n",
    "def embed_system(R_G, R_H, params):\n",
    "    # Process parameters\n",
    "    dimensions, walk_length, num_walks = params[\"dimensions\"], params[\"walk_length\"], params[\"num_walks\"]\n",
    "    workers, window, min_count, batch_words = params[\"workers\"], params[\"window\"], params[\"min_count\"], params[\"batch_words\"]\n",
    "\n",
    "    # Generate walks\n",
    "    R_G_emb = N2V(R_G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=workers, quiet=True).fit(window=window, min_count=min_count, batch_words=batch_words)\n",
    "    R_H_emb = N2V(R_H, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=workers, quiet=True).fit(window=window, min_count=min_count, batch_words=batch_words)\n",
    "\n",
    "    # Retrieve embedded models\n",
    "    G_ = R_G_emb.wv\n",
    "    H_ = R_H_emb.wv\n",
    "\n",
    "    return G_, H_\n",
    "\n",
    "def reconstruct_system(testset, G, H, G_, H_, params):\n",
    "    cls = []\n",
    "    scores = []\n",
    "    gt = []\n",
    "\n",
    "    _dot = lambda x, y: np.exp(-1*np.dot(np.transpose(x), y))\n",
    "\n",
    "    for edge, gt_ in testset.items():\n",
    "        i, j = edge\n",
    "        gt.append(gt_)\n",
    "\n",
    "        v_G_i = G_[i]\n",
    "        v_G_j = G_[j]\n",
    "        v_H_i = H_[i]\n",
    "        v_H_j = H_[j]\n",
    "\n",
    "        if params[\"metric\"] == \"logistic\":\n",
    "            d_G = 1 / (1 + _dot(v_G_i, v_G_j))\n",
    "            d_H = 1 / (1 + _dot(v_H_i, v_H_j))\n",
    "        else:\n",
    "            d_G = np.linalg.norm(v_G_i - v_G_j)\n",
    "            d_H = np.linalg.norm(v_H_i - v_H_j)\n",
    "\n",
    "            if params[\"metric\"] == \"inverse\":\n",
    "                d_G = 1 / (d_G + 1e-16)\n",
    "                d_H = 1 / (d_H + 1e-16)\n",
    "            elif params[\"metric\"] == \"negexp\":\n",
    "                d_G = np.exp(-d_G)\n",
    "                d_H = np.exp(-d_H)\n",
    "\n",
    "        t_G = d_G / (d_G + d_H)\n",
    "        t_H = 1 - t_G\n",
    "\n",
    "        scores.append(t_G)\n",
    "\n",
    "        cls_ = np.random.randint(2)\n",
    "        if t_G != t_H:\n",
    "            if np.random.rand() <= t_G:\n",
    "                cls_ = 1\n",
    "            else:\n",
    "                cls_ = 0\n",
    "        cls.append(cls_)\n",
    "\n",
    "    return cls, scores, gt\n",
    "\n",
    "def measure_performance(cls, scores, gt):\n",
    "    # print(f\"Classifications: {cls}\\n Scores: {scores}\\n GT: {gt}\")\n",
    "    acc = accuracy(gt, cls)\n",
    "    auc = auroc(gt, scores)\n",
    "\n",
    "    return acc, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_grid():\n",
    "    # Initialize parameter grid\n",
    "    grid = []\n",
    "\n",
    "    # Initialize grid vertex archetype\n",
    "    params = {}\n",
    "\n",
    "    # Fill in static parameters (mostly embedding)\n",
    "    params.update({\n",
    "        \"largest_component\": True,\n",
    "        \"dimensions\": 128,\n",
    "        \"walk_length\": 30,\n",
    "        \"num_walks\": 100,\n",
    "        \"workers\": 8,\n",
    "        \"window\": 10,\n",
    "        \"min_count\": 1,\n",
    "        \"batch_words\": 4\n",
    "    })\n",
    "\n",
    "\n",
    "    # Fill in each vertex with dynamic parameters (networks, mostly)\n",
    "    for repetition in range(1):\n",
    "        for system in [\"celegans\", \"london\"]: #[\"arxiv\", \"celegans\", \"drosophila\", \"london\"]:\n",
    "            for metric in [\"inverse\", \"negexp\"]:\n",
    "                for pfi in [0.10, 0.25, 0.50, 0.75, 0.95]:\n",
    "                    if system == \"arxiv\":\n",
    "                        _range = range(1,14)\n",
    "                    elif system == \"celegans\":\n",
    "                        _range = range(1,4)\n",
    "                    elif system == \"drosophila\":\n",
    "                        _range = range(1,8)\n",
    "                    elif system == \"london\":\n",
    "                        _range = range(1,4)\n",
    "\n",
    "                    for left in _range:\n",
    "                        for right in range(1,left):\n",
    "                            params_ = deepcopy(params)\n",
    "\n",
    "                            params_[\"system\"] = system\n",
    "                            params_[\"left\"] = left\n",
    "                            params_[\"right\"] = right\n",
    "\n",
    "                            params_[\"metric\"] = metric\n",
    "                            params_[\"pfi\"] = pfi\n",
    "                            params_[\"repetition\"] = repetition\n",
    "\n",
    "                            grid.append(params_)\n",
    "\n",
    "    return grid\n",
    "\n",
    "def workflow():\n",
    "    # Process parameter grid\n",
    "    grid = _process_grid()\n",
    "\n",
    "    # Prepare results dataframe (dict)\n",
    "    df_ = {key: [] for key in grid[0].keys()}\n",
    "    df_[\"Accuracy\"] = []\n",
    "    df_[\"AUROC\"] = []\n",
    "\n",
    "    # Run experiment\n",
    "    for gv in tqdm(grid, desc=\"Experiment\", colour=\"purple\"):\n",
    "        # Track parameters\n",
    "        for key, value in gv.items():\n",
    "            df_[key].append(value)\n",
    "\n",
    "        # Generate synthetic system and form remnants\n",
    "        G, H, R_G, R_H, agg = format_system(gv)\n",
    "\n",
    "        # Apply node2vec\n",
    "        R_G_embedding, R_H_embedding = embed_system(R_G, R_H, gv)\n",
    "\n",
    "        # Retrieve vectors from embedding model\n",
    "        # * NOTE: There is some random reindexing occuring\n",
    "        # * We use explicit index hash map here to avoid making embeddings incomparable\n",
    "        R_G_vectors = {\n",
    "            int(R_G_embedding.index_to_key[i]) : R_G_embedding.vectors[i]\n",
    "            for i in range(G.number_of_nodes())\n",
    "        }\n",
    "        R_H_vectors = {\n",
    "            int(R_H_embedding.index_to_key[i]) : R_H_embedding.vectors[i]\n",
    "            for i in range(H.number_of_nodes())\n",
    "        }\n",
    "\n",
    "        # Reconstruct system from embeddings\n",
    "        cls, scores, gt = reconstruct_system(agg, G, H, R_G_vectors, R_H_vectors, gv)\n",
    "\n",
    "        # Measure performance\n",
    "        acc, auc = measure_performance(cls, scores, gt)\n",
    "\n",
    "        # Update performance measure columns\n",
    "        df_[\"Accuracy\"].append(acc)\n",
    "        df_[\"AUROC\"].append(auc)\n",
    "\n",
    "    # Format dataframe and save\n",
    "    df = pd.DataFrame(df_)\n",
    "    df.to_parquet(f\"{df_template}.parquet\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "df = workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('EmbeddedNaive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5825d70345ee2a3649307867a940362a6242bb96acddd38c1a628ab339f97fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
